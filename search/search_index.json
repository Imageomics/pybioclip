{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"pybioclip","text":"<p>Command line tool and python package to simplify the use of BioCLIP and  BioCLIP 2 (default).</p> <p>Key features include:</p> <ul> <li>Taxonomic label prediction for images across ranks in the Linnaean hierarchy (tunable from kingdom to species).</li> <li>Custom label predictions from user-supplied classification categories.</li> <li>Image embedding generation in a text-aligned feature space.</li> <li>Batch image processing with performance optimizations.</li> <li>Containers provided to simplfy use in computational pipelines.</li> </ul> <p>No particular coding knowledge of ML or computer vision is required to use pybioclip.</p>"},{"location":"#installation","title":"Installation","text":"<p>Requires python that is compatible with PyTorch.</p> <p><pre><code>pip install pybioclip\n</code></pre> If you have any issues with installation, please first upgrade pip by running <code>pip install --upgrade pip</code>.</p>"},{"location":"#tutorials","title":"Tutorials","text":"<p>Command Line Tutorial Python Tutorial</p>"},{"location":"acknowledgments/","title":"Acknowledgments","text":"<p>The Imageomics Institute is supported by the National Science Foundation under Award No. 2118240 \"HDR Institute: Imageomics: A New Frontier of Biological Information Powered by Knowledge-Guided Machine Learning.\" Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.</p> <p>The prediction code in pybioclip is based on work by @samuelstevens in bioclip-demo.</p>"},{"location":"apptainer/","title":"Apptainer Container","text":"<p>Apptainer/Singularity images for pybioclip are provided at ghcr.io/Imageomics/pybioclip-sif registry.</p> <p>NOTE: It is also possible to download the pybioclip docker container and convert that into a singularity container, but that process can take quite a while.</p>"},{"location":"apptainer/#tutorial","title":"Tutorial","text":""},{"location":"apptainer/#download-example-images","title":"Download example images","text":"<p>Download two images from the bioclip-demo.</p> <pre><code>wget https://huggingface.co/spaces/imageomics/bioclip-demo/resolve/main/examples/Ursus-arctos.jpeg\nwget https://huggingface.co/spaces/imageomics/bioclip-demo/resolve/main/examples/Felis-catus.jpeg\n</code></pre>"},{"location":"apptainer/#download-a-pybioclip-container","title":"Download a pybioclip container","text":"<p><pre><code>apptainer pull oras://ghcr.io/imageomics/pybioclip-sif:1.0.0\n</code></pre> The above command will create a <code>pybioclip_1.0.0.sif</code> container image file.</p>"},{"location":"apptainer/#create-predictions-using-a-cpu","title":"Create predictions using a CPU","text":"<pre><code>./pybioclip_sif_1.0.0.sif bioclip predict Ursus-arctos.jpeg Felis-catus.jpeg\n</code></pre>"},{"location":"apptainer/#create-predictions-using-a-gpu","title":"Create predictions using a GPU","text":"<p>This step requires a cuda GPU.</p> <pre><code>apptainer exec -nv ./pybioclip_sif_1.0.0.sif bioclip predict --device cuda Ursus-arctos.jpeg Felis-catus.jpeg\n</code></pre>"},{"location":"apptainer/#create-predictions-using-a-gpu-via-a-slurm-job","title":"Create predictions using a GPU via a Slurm Job","text":"<p>This step requires being on a Slurm cluster.</p> <p>Create a Slurm sbatch script named <code>bioclip.sh</code> with the following content: <pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --time=00:10:00\n#SBATCH --gpus-per-node=1\napptainer exec --nv ./pybioclip_sif_1.0.0.sif bioclip predict --device cuda $*\n</code></pre> Run the slurm job filling in your Slurm account: <pre><code>sbatch --account &lt;SLURMACCT&gt; bioclip.sh Ursus-arctos.jpeg Felis-catus.jpeg\n</code></pre></p>"},{"location":"command-line-help/","title":"Command Line Help","text":""},{"location":"command-line-help/#bioclip-predict","title":"bioclip predict","text":"<p>Use BioCLIP to generate predictions for image files. <pre><code>usage: bioclip predict [-h] [--format {table,csv}] [--output OUTPUT]\n                       [--rank {kingdom,phylum,class,order,family,genus,species} |\n                        --cls CLS | --bins BINS | --subset SUBSET] [--k K]\n                       [--device DEVICE] [--model MODEL] [--pretrained PRETRAINED]\n                       [--batch-size BATCH_SIZE]\n                       image_file [image_file ...]\n\npositional arguments:\n  image_file            input image file(s)\n\noptions:\n  -h, --help            show this help message and exit\n  --format {table,csv}  format of the output, default: csv\n  --output OUTPUT       print output to file, default: stdout\n  --rank {kingdom,phylum,class,order,family,genus,species}\n                        rank of the classification, default: species, when\n                        specified the --cls, --bins, and --subset arguments\n                        are not allowed.\n  --cls CLS             classes to predict: either a comma separated list or a\n                        path to a text file of classes (one per line), when\n                        specified the --rank, --bins, and --subset arguments\n                        are not allowed.\n  --bins BINS           path to CSV file with two columns with the first being\n                        classes and second being bin names, when specified the\n                        --rank, --cls, and --subset arguments are not allowed.\n  --subset SUBSET       path to CSV file used to subset the tree of life\n                        embeddings. CSV first column must be named one of\n                        kingdom,phylum,class,order,family,genus,species. When\n                        specified the --rank, --bins, and --cls arguments are\n                        not allowed.\n  --k K                 number of top predictions to show, default: 5\n  --device DEVICE       device to use (cpu or cuda or mps), default: cpu\n  --model MODEL         model identifier (see command list-models);\n                        default: hf-hub:imageomics/bioclip-2\n  --pretrained PRETRAINED\n                        pretrained model checkpoint as tag or file, depends on\n                        model; needed only if more than one is available\n                        (see command list-models)\n  --batch-size BATCH_SIZE\n                        Number of images to process in a batch, default: 10\n  --log LOG_FILE        Path to a file for recording prediction logs.\n                        If the file extension is '.json', the log is written\n                        in JSON for building a provenance chain; otherwise, \n                        logs are appended in a human-readable text format.\n                        If not specified, no log is written.\n</code></pre></p>"},{"location":"command-line-help/#bioclip-embed","title":"bioclip embed","text":"<p>Use BioCLIP to generate embeddings for image files. <pre><code>usage: bioclip embed [-h] [--output OUTPUT] [--device DEVICE] [--model MODEL]\n                     [--pretrained PRETRAINED] image_file [image_file ...]\n\npositional arguments:\n  image_file            input image file(s)\n\noptions:\n  -h, --help            show this help message and exit\n  --output OUTPUT       print output to file, default: stdout\n  --device DEVICE       device to use (cpu or cuda or mps), default: cpu\n  --model MODEL         model identifier (see command list-models);\n                        default: hf-hub:imageomics/bioclip-2\n  --pretrained PRETRAINED\n                        pretrained model checkpoint as tag or file, depends\n                        on model; needed only if more than one is available\n                        (see command list-models)\n</code></pre></p>"},{"location":"command-line-help/#bioclip-list-models","title":"bioclip list-models","text":"<p>List available models and pretrained model checkpoints. <pre><code>usage: bioclip list-models [-h] [--model MODEL]\n\nNote that this will only list models known to open_clip; any model identifier\nloadable by open_clip, such as from hf-hub, file, etc should also be usable for\n--model in the embed and predict commands.\n(The default model hf-hub:imageomics/bioclip-2 is one example.)\n\noptions:\n  -h, --help     show this help message and exit\n  --model MODEL  list available tags for pretrained model checkpoint(s) for\n                 specified model\n</code></pre></p>"},{"location":"command-line-help/#bioclip-list-tol-taxa","title":"bioclip list-tol-taxa","text":"<p>Print a CSV of the taxa embedding labels included with the tree of life model to the terminal. <pre><code>usage: bioclip list-tol-taxa [-h]\n\noptions:\n  -h, --help  show this help message and exit\n</code></pre></p>"},{"location":"command-line-tutorial/","title":"Command Line Tutorial","text":"<p>Before beginning this tutorial you need to install pybioclip and download two example images: <code>Ursus-arctos.jpeg</code> and <code>Felis-catus.jpeg</code>.</p>"},{"location":"command-line-tutorial/#tree-of-life-predictions","title":"Tree Of Life Predictions","text":"<p>The <code>bioclip predict</code> command, when not supplying a custom list of labels, will create a prediction based on the BioCLIP 2 TreeOfLife-200M embeddings.</p>"},{"location":"command-line-tutorial/#predict-species-for-an-image","title":"Predict species for an image","text":"<p>Predict species for an <code>Ursus-arctos.jpeg</code> file: <pre><code>bioclip predict Ursus-arctos.jpeg\n</code></pre> Output: <pre><code>file_name,kingdom,phylum,class,order,family,genus,species_epithet,species,common_name,score\nUrsus-arctos.jpeg,Animalia,Chordata,Mammalia,Carnivora,Ursidae,Ursus,arctos,Ursus arctos,Kodiak bear,0.9356034994125366\nUrsus-arctos.jpeg,Animalia,Chordata,Mammalia,Carnivora,Ursidae,Ursus,arctos syriacus,Ursus arctos syriacus,syrian brown bear,0.05616999790072441\nUrsus-arctos.jpeg,Animalia,Chordata,Mammalia,Carnivora,Ursidae,Ursus,arctos bruinosus,Ursus arctos bruinosus,,0.004126196261495352\nUrsus-arctos.jpeg,Animalia,Chordata,Mammalia,Carnivora,Ursidae,Ursus,arctus,Ursus arctus,,0.0024959812872111797\nUrsus-arctos.jpeg,Animalia,Chordata,Mammalia,Carnivora,Ursidae,Ursus,americanus,Ursus americanus,Louisiana black bear,0.0005009894957765937\n</code></pre></p> <p>Documentation</p> <p>The bioclip predict documentation describes all arguments supported by <code>bioclip predict</code> command.</p>"},{"location":"command-line-tutorial/#predict-species-for-multiple-images-saving-to-a-file","title":"Predict species for multiple images saving to a file","text":"<p>To make predictions for files <code>Ursus-arctos.jpeg</code> and <code>Felis-catus.jpeg</code> saving the output to a file named <code>predictions.csv</code>: <pre><code>bioclip predict --output predictions.csv Ursus-arctos.jpeg Felis-catus.jpeg\n</code></pre> The contents of <code>predictions.csv</code> will look like this:  <pre><code>file_name,kingdom,phylum,class,order,family,genus,species_epithet,species,common_name,score\nUrsus-arctos.jpeg,Animalia,Chordata,Mammalia,Carnivora,Ursidae,Ursus,arctos,Ursus arctos,Kodiak bear,0.9356034994125366\nUrsus-arctos.jpeg,Animalia,Chordata,Mammalia,Carnivora,Ursidae,Ursus,arctos syriacus,Ursus arctos syriacus,syrian brown bear,0.05616999790072441\nUrsus-arctos.jpeg,Animalia,Chordata,Mammalia,Carnivora,Ursidae,Ursus,arctos bruinosus,Ursus arctos bruinosus,,0.004126196261495352\nUrsus-arctos.jpeg,Animalia,Chordata,Mammalia,Carnivora,Ursidae,Ursus,arctus,Ursus arctus,,0.0024959812872111797\nUrsus-arctos.jpeg,Animalia,Chordata,Mammalia,Carnivora,Ursidae,Ursus,americanus,Ursus americanus,Louisiana black bear,0.0005009894957765937\nFelis-catus.jpeg,Animalia,Chordata,Mammalia,Carnivora,Felidae,Felis,silvestris,Felis silvestris,European Wildcat,0.7221033573150635\nFelis-catus.jpeg,Animalia,Chordata,Mammalia,Carnivora,Felidae,Felis,catus,Felis catus,Domestic Cat,0.19810837507247925\nFelis-catus.jpeg,Animalia,Chordata,Mammalia,Carnivora,Felidae,Felis,margarita,Felis margarita,Sand Cat,0.02798456884920597\nFelis-catus.jpeg,Animalia,Chordata,Mammalia,Carnivora,Felidae,Lynx,felis,Lynx felis,,0.021829601377248764\nFelis-catus.jpeg,Animalia,Chordata,Mammalia,Carnivora,Felidae,Felis,bieti,Felis bieti,Chinese desert cat,0.010979168117046356\n</code></pre></p> <p>Documentation</p> <p>The bioclip predict documentation describes all arguments supported by <code>bioclip predict</code> command.</p>"},{"location":"command-line-tutorial/#predict-top-3-genera-for-an-image-and-display-output-as-a-table","title":"Predict top 3 genera for an image and display output as a table","text":"<pre><code>bioclip predict --format table --k 3 --rank=genus Ursus-arctos.jpeg\n</code></pre> <p>Output: <pre><code>+-------------------+----------+----------+----------+--------------+----------+--------+------------------------+\n|     file_name     | kingdom  |  phylum  |  class   |    order     |  family  | genus  |         score          |\n+-------------------+----------+----------+----------+--------------+----------+--------+------------------------+\n| Ursus-arctos.jpeg | Animalia | Chordata | Mammalia |  Carnivora   | Ursidae  | Ursus  |   0.9994320273399353   |\n| Ursus-arctos.jpeg | Animalia | Chordata | Mammalia | Artiodactyla | Cervidae | Cervus | 0.00032594642834737897 |\n| Ursus-arctos.jpeg | Animalia | Chordata | Mammalia | Artiodactyla | Cervidae | Alces  | 7.803700282238424e-05  |\n+-------------------+----------+----------+----------+--------------+----------+--------+------------------------+\n</code></pre></p> <p>Documentation</p> <p>The bioclip predict documentation describes all arguments supported by <code>bioclip predict</code> command.</p>"},{"location":"command-line-tutorial/#predict-using-a-tol-subset","title":"Predict using a TOL subset","text":"<p>The <code>predict</code> command has support for a <code>--subset &lt;csv-path&gt;</code> argument. The first column in the CSV file must be named kingdom, phylum, class, order, family, genus, or species. The values must match the TOL labels otherwise an error occurs. See the bioclip list-tol-taxa command to create a CSV of TOL labels.</p> <p>In this example we create a CSV to subset TOL to two orders. Create a CSV named <code>orders.csv</code> with the following content: <pre><code>order\nArtiodactyla\nRodentia\n</code></pre></p> <pre><code>bioclip predict --subset orders.csv Ursus-arctos.jpeg\n</code></pre> <p>Output: <pre><code>file_name,kingdom,phylum,class,order,family,genus,species_epithet,species,common_name,score\nUrsus-arctos.jpeg,Animalia,Chordata,Mammalia,Artiodactyla,Cervidae,Cervus,canadensis sibericus,Cervus canadensis sibericus,,0.7347981333732605\nUrsus-arctos.jpeg,Animalia,Chordata,Mammalia,Artiodactyla,Cervidae,Alces,alces,Alces alces,European elk,0.17302176356315613\n...\n</code></pre></p> <p>Documentation</p> <p>The bioclip predict documentation describes all arguments supported by <code>bioclip predict</code> command.</p>"},{"location":"command-line-tutorial/#use-original-bioclip-model","title":"Use Original BioCLIP Model","text":"<p>By default the BioCLIP 2 model is used. The original BioCLIP model can be used by including a <code>--model hf-hub:imageomics/bioclip</code> argument for either the <code>predict</code> or <code>embed</code> commands. Example: <pre><code>bioclip predict --model hf-hub:imageomics/bioclip Ursus-arctos.jpeg\n</code></pre></p> <p>When using the original BioCLIP model for TreeOfLife predictions the TreeOfLife-10M embeddings are used.</p>"},{"location":"command-line-tutorial/#save-a-prediction-log","title":"Save a prediction log","text":"<p>The <code>predict</code> command supports a <code>--log &lt;path&gt;</code> argument to save prediction details to a file. If the file extension is '.json', the log is written in JSON for building a provenance chain; otherwise, logs are appended in a human-readable text format. By default no log is written.</p> <p>Example usage: <pre><code>bioclip predict --log bioclip.json Ursus-arctos.jpeg\n</code></pre></p> <p>The resulting <code>bioclip.json</code> will contain structured information similar to: <pre><code>{\n    \"pybioclip_version\": \"1.3.3\",\n    \"start\": \"2025-07-08T16:25:11\",\n    \"end\": \"2025-07-08T16:25:13\",\n    \"command_line\": \"bioclip predict --log bioclip.json Ursus-arctos.jpeg\",\n    \"model\": \"hf-hub:imageomics/bioclip-2\",\n    \"pretrained\": null,\n    \"device\": \"cpu\",\n    \"top_level_settings\": {\n        \"tree_of_life_version\": \"imageomics/TreeOfLife-200M\",\n        \"subset\": null\n    },\n    \"predictions\": [\n        {\n            \"images\": [\n                \"Ursus-arctos.jpeg\"\n            ],\n            \"details\": {\n                \"rank\": \"species\",\n                \"min_prob\": 1e-09,\n                \"k\": 5,\n                \"batch_size\": 10\n            }\n        }\n    ]\n}\n</code></pre></p>"},{"location":"command-line-tutorial/#custom-label-predictions","title":"Custom Label Predictions","text":"<p>To predict with custom labels using the <code>bioclip predict</code> command the <code>--cls</code> or <code>--bins</code> arguments must be used.</p>"},{"location":"command-line-tutorial/#predict-from-a-list-of-classes","title":"Predict from a list of classes","text":"<p>Create predictions for 3 classes (cat, bird, and bear) for image <code>Ursus-arctos.jpeg</code>: <pre><code>bioclip predict --cls cat,bird,bear Ursus-arctos.jpeg\n</code></pre> Output: <pre><code>file_name,classification,score\nUrsus-arctos.jpeg,bear,0.9999998807907104\nUrsus-arctos.jpeg,cat,4.581697155003894e-08\nUrsus-arctos.jpeg,bird,3.052039332374079e-08\n</code></pre></p> <p>Documentation</p> <p>The bioclip predict documentation describes all arguments supported by <code>bioclip predict</code> command.</p>"},{"location":"command-line-tutorial/#predict-from-a-binning-csv","title":"Predict from a binning CSV","text":"<p>Create predictions for 3 classes (cat, bird, and bear) with 2 bins (one, two) for image <code>Ursus-arctos.jpeg</code>:</p> <p>Create a CSV file named <code>bins.csv</code> with the following contents: <pre><code>cls,bin\ncat,one\nbird,one\nbear,two\n</code></pre> The names of the columns do not matter. The first column values will be used as the classes. The second column values will be used for bin names.</p> <p>Run predict command: <pre><code>bioclip predict --bins bins.csv Ursus-arctos.jpeg\n</code></pre></p> <p>Output: <pre><code>Ursus-arctos.jpeg,two,0.9999998807907104\nUrsus-arctos.jpeg,one,7.633736487377973e-08\n</code></pre></p> <p>Documentation</p> <p>The bioclip predict documentation describes all arguments supported by <code>bioclip predict</code> command.</p>"},{"location":"command-line-tutorial/#predict-using-a-custom-model","title":"Predict using a custom model","text":"<p>List available models: <pre><code>bioclip list-models\n</code></pre> Output: <pre><code>...\nViT-B-16\nViT-B-16-plus\n...\n</code></pre> List pretrained models for a model: <pre><code>bioclip list-models --model  ViT-B-16\n</code></pre> Output: <pre><code>...\nlaion2b_s34b_b88k\n...\n</code></pre> Create a prediction: <pre><code>bioclip predict --cls duck,fish,bear --model ViT-B-16 --pretrained laion2b_s34b_b88k Ursus-arctos.jpeg\n</code></pre> Output: <pre><code>file_name,classification,score\nUrsus-arctos.jpeg,bear,0.9999988079071045\nUrsus-arctos.jpeg,fish,1.1098603636128246e-06\nUrsus-arctos.jpeg,duck,2.7479762465532076e-08\n</code></pre></p>"},{"location":"command-line-tutorial/#create-embeddings","title":"Create embeddings","text":"<p>The <code>bioclip embed</code> command creates an embedding for one or more image files.</p>"},{"location":"command-line-tutorial/#create-embedding-for-an-image","title":"Create embedding for an image","text":"<p><pre><code>bioclip embed Ursus-arctos.jpeg\n</code></pre> Output: <pre><code>{\n    \"model\": \"hf-hub:imageomics/bioclip-2\",\n    \"embeddings\": {\n        \"Ursus-arctos.jpeg\": [\n            -0.23633578419685364,\n            -0.28467196226119995,\n            -0.4394485652446747,\n            ...\n        ]\n    }\n}\n</code></pre></p> <p>Documentation</p> <p>The bioclip embed documentation describes all arguments supported by <code>bioclip embed</code> command.</p>"},{"location":"command-line-tutorial/#view-command-line-help","title":"View command line help","text":"<pre><code>bioclip -h\nbioclip &lt;command&gt; -h\n</code></pre>"},{"location":"docker/","title":"Docker Container","text":"<p>A docker container for pybioclip is hosted at ghcr.io. This container has CPU support for Mac, Windows, and Linux. GPU(CUDA) support is only for Windows and Linux.</p>"},{"location":"docker/#volumes","title":"Volumes","text":"<p>In order to access files the docker container requires you to mount a volume. The working and home directories in this container are both set to <code>/home/bcuser</code>. Minimally you need to mount a volume into this directory so pybioclip can read your images. When running pybioclip the software will download various BioCLIP files into a <code>/home/bcuser/.cache</code> subdirectory. If you want to store the <code>.cache</code> folder in your home directory you will need to mount that directory (<code>~/.cache</code>) into the container at <code>/home/bcuser/.cache</code>.</p>"},{"location":"docker/#setup","title":"Setup","text":"<p>The examples below require an image file named Ursus-arctos.jpeg in the current directory.</p>"},{"location":"docker/#maclinux-cpu-usage","title":"Mac/Linux CPU Usage","text":"<p>The following command will create predictions for the <code>Ursus-arctos.jpeg</code> image in the current directory. The command mounts the current directory into the container at <code>/home/bcuser</code>. The command mounts the <code>~/.cache</code> directory into the container to cache BioCLIP files in your home directory. <pre><code>docker run --platform linux/amd64 \\\n           -v $(pwd):/home/bcuser \\\n           -v ~/.cache:/home/bcuser/.cache \\\n           --rm ghcr.io/imageomics/pybioclip:1.1.0 \\\n           bioclip predict Ursus-arctos.jpeg\n</code></pre></p>"},{"location":"docker/#linux-gpu-usage","title":"Linux GPU Usage","text":"<p>The following command will create predictions using a GPU for the <code>Ursus-arctos.jpeg</code> image in the current directory. <pre><code>docker run --gpus all \\\n           --platform linux/amd64 \\\n           -v $(pwd):/home/bcuser \\\n           -v ~/.cache:/home/bcuser/.cache \\\n           --rm ghcr.io/imageomics/pybioclip:1.1.0 \\\n           bioclip predict --device cuda Ursus-arctos.jpeg\n</code></pre></p>"},{"location":"docker/#windows-cpu-usage","title":"Windows CPU Usage","text":"<p>The following command will create predictions for the <code>Ursus-arctos.jpeg</code> image in the current directory. Since this command does not mount <code>/home/bcuser/.cache</code> in the container the <code>.cache</code> directory will be created within the current directory. <pre><code>docker run --rm -v %cd%:/home/bcuser ghcr.io/imageomics/pybioclip:1.0.0 bioclip predict Ursus-arctos.jpeg\n</code></pre></p>"},{"location":"docker/#windows-gpu-usage","title":"Windows GPU Usage","text":"<p>The following command will create predictions using a GPU for the <code>Ursus-arctos.jpeg</code> image in the current directory. <pre><code>docker run --rm --gpus all -v %cd%:/home/bcuser ghcr.io/imageomics/pybioclip bioclip:1.0.0 predict --device cuda Ursus-arctos.jpeg\n</code></pre></p>"},{"location":"geo-restricted-taxa/","title":"Improving Prediction Accuracy","text":""},{"location":"geo-restricted-taxa/#prediction-using-a-geographically-restricted-list","title":"Prediction Using a Geographically Restricted List","text":"<p><code>pybioclip</code> provides three features for making predictions using a restricted set of terms:</p> <ul> <li><code>--cls</code></li> <li><code>--bins</code></li> <li><code>--subset</code></li> </ul> <p>One way to populate the restricted set of terms is to use taxa that are known to have been observed in a specific geographic region.</p> <p>There are many ways to obtain a list with this constraint. Two noteworthy methods are:</p> <ul> <li>Using the Global Biodiversity Information Facility (GBIF) to download a list of taxa that have been observed in a specific geographic region.</li> <li>Using Map of Life (MOL) to download a list of taxa that have been observed in a specific geographic region.</li> </ul> <p>In our examples, we will use the image below of the n\u0113n\u0113, or the Hawaiian goose (Branta sandvicensis). Its full taxonomic classification is Animalia Chordata Aves Anseriformes Anatidae Branta sandvicensis. </p> <p>Save this image to your working directory as <code>nene.jpg</code> for the example.</p> <p></p> <p>\u00a9 Kevin Schafer</p>"},{"location":"geo-restricted-taxa/#getting-a-geographically-restricted-list-of-taxa","title":"Getting a Geographically Restricted List of Taxa","text":"<p>Both GBIF and MOL provide web interfaces that enable users to produce a species list filtered by geographic coordinates and regions. A brief tutorial is provided. These examples illustrate a small subset of the capabilities provided by these platforms.</p>"},{"location":"geo-restricted-taxa/#gbif-web-interface-tutorial","title":"GBIF Web Interface Tutorial","text":"<p>You must have an account with GBIF to create downloads with a DOI. You do not need an account to download data with a DOI created by others.</p> <p>Visit https://www.gbif.org/ and click <code>Login</code> and <code>Register</code> to create an account.</p> <p>Once you have an account, log in to manage data downloads from your profile.</p> <p>Example: filter for all occurrences of birds in the islands of Hawai'i with GBIF.</p> <p>Note</p> <p>Rather than creating a new download, you may use https://doi.org/10.15468/dl.469rtz, which was prepared for this example using the steps below. The information in this download may be different from a download you create, as more data is being added to GBIF regularly. Taxa have been extracted from the species list in this download and are available in:</p> <ul> <li>hawaii_bird_species_list.txt</li> <li>hawaii_bird_families_list.txt</li> <li>hawaii_bird_family_bins_list.csv (terms for bins of families were created manually)</li> </ul> <p>You can save these files to your working directory for the example.</p> <ol> <li>Log in to https://www.gbif.org/ and click <code>OCCURRENCES</code> near the search bar.</li> <li>Apply filters for your search (you can apply the filters in any combination and order).<ul> <li>To filter by region, use <code>Location</code> with <code>Including coordinates</code>, <code>Administrative areas (gadm.org)</code>, <code>Country or area</code>, or <code>Continent</code>.</li> <li>The <code>Location</code> feature is very flexible. You may use the built-in map widget to specify one or more regions with polygons or squares by hand. You may also specify one or more square ranges by entering precise coordinates into <code>Range</code> and/or <code>Geometry</code>. These features are able to be used in combination.</li> <li>Under the <code>Scientific name</code> filter, type \"Aves\", and select \"Aves Class\" from the search results.</li> <li>Under the <code>Location</code> filter, click <code>Geometry</code>, and enter the following into the \"WKT or GeoJSON\" field to specify the main eight islands of Hawai'i: <pre><code>POLYGON((-160.54 18.54, -154.46 18.54, -154.46 22.26, -160.54 22.26, -160.54 18.54))\n</code></pre><ul> <li>You could also paste the equivalent raw content of the file hawaii.geojson into the \"WKT or GeoJSON\" field under <code>Geometry</code>.</li> </ul> </li> <li>Click <code>ADD</code></li> </ul> </li> <li>Download the data.<ul> <li>Once the query completes, click <code>DOWNLOAD</code>.</li> <li>Select the <code>SPECIES LIST</code>, read and agree to the terms of use, and click <code>UNDERSTOOD</code>.</li> <li>The download will be prepared and associated with the DOI displayed at the top of the page, and you will receive an email when it is ready. </li> <li>The CSV will contain a list of taxa that meet the requirements of the filter with contents as defined by the GBIF \"Species list\" download format.</li> <li>You may extract contents of the CSV to a text file as you see fit for use with <code>pybioclip</code>, as we have with <code>hawaii_bird_species_list.txt</code>, <code>hawaii_bird_families_list.txt</code>, and <code>hawaii_bird_family_bins_list.csv</code>.</li> </ul> </li> </ol>"},{"location":"geo-restricted-taxa/#mol-web-interface-tutorial","title":"MOL Web Interface Tutorial","text":"<p>There are no prerequisites. An account with MOL is optional and not required for downloading data. </p> <p>Note that any URL shared for a filter applied to MOL will yield filter results as of the date the URL is used (rather than the date you applied the filter). You should host your own copy of the data if you need to ensure the data is static.</p> <p>Example: filter for all species of birds in the islands of Hawai'i with MOL.</p> <ol> <li>Visit https://mol.org/ and click <code>Regions</code>.</li> <li>Under \"Search for an area by name:\", type \"Hawai'i\" and select \"Hawai'i, United States\".</li> <li>Click <code>Go to Species Report</code>. This should take you to a page matching this URL.</li> <li>Under \"Species\", click <code>&lt;N&gt; Birds</code>, where <code>&lt;N&gt;</code> is the number of bird species in the region. This should append <code>&amp;group=birds</code> to the URL.</li> <li>Refresh the browser to apply the Birds filter to the download. The filter will be applied when loading the URL with the Birds filter applied. Note that if you download before refreshing, the download will include all species in the region.</li> <li>Click <code>Download</code>, review the information in the pop-up about the download contents, and click <code>Download Now</code>.</li> </ol>"},{"location":"geo-restricted-taxa/#examples-using-a-geographically-restricted-list-of-taxa-for-prediction","title":"Examples Using a Geographically Restricted List of Taxa for Prediction","text":"<p>These examples use the files containing information extracted from the GBIF download. You are encouraged to practice by creating similarly formatted files with the <code>scientific_name</code> and <code>family</code> columns from the <code>SpeciesList.csv</code> file in the MOL download.</p> <p>Example: Predict the species of an image using open-ended classification.</p> <p><pre><code>bioclip predict --k 5 --format table nene.jpg\n</code></pre> Output: <pre><code>+-----------+----------+----------+-------+--------------+----------+--------+-----------------+---------------------+----------------+---------------------+\n| file_name | kingdom  |  phylum  | class |    order     |  family  | genus  | species_epithet |       species       |  common_name   |        score        |\n+-----------+----------+----------+-------+--------------+----------+--------+-----------------+---------------------+----------------+---------------------+\n|  nene.jpg | Animalia | Chordata |  Aves |  Gruiformes  | Gruidae  |  Grus  |       grus      |      Grus grus      |  Common Crane  |  0.7501388788223267 |\n|  nene.jpg | Animalia | Chordata |  Aves |  Gruiformes  | Gruidae  |  Grus  |     communis    |    Grus communis    |                | 0.07198520749807358 |\n|  nene.jpg | Animalia | Chordata |  Aves | Anseriformes | Anatidae | Branta |   sandvicensis  | Branta sandvicensis | Hawaiian Goose | 0.04125870019197464 |\n|  nene.jpg | Animalia | Chordata |  Aves |  Gruiformes  | Gruidae  |  Grus  |     cinerea     |     Grus cinerea    |                | 0.03031359612941742 |\n|  nene.jpg | Animalia | Chordata |  Aves |  Gruiformes  | Gruidae  |  Grus  |     monacha     |     Grus monacha    |  Hooded Crane  | 0.02994249388575554 |\n+-----------+----------+----------+-------+--------------+----------+--------+-----------------+---------------------+----------------+---------------------+\n</code></pre></p> <p>Incorrect top predictions.</p> <p>The top two predictions are incorrect. This suggests that filtering the possibilities for predicted labels may improve the accuracy of the predictions.</p> <p>We can imagine that all we know about the image is that it was taken on one of the Hawaiian islands. We can use multiple approaches incorporating the geographically restricted lists to make a more informed decision. </p> <p>Example: Predict the species of an image using the geographically restricted list of species directly as custom labels.</p> <p>Using hawaii_bird_species_list.txt: <pre><code>bioclip predict --k 3 --format table --cls hawaii_bird_species_list.txt nene.jpg\n</code></pre> Output: <pre><code>+-----------+---------------------+---------------------+\n| file_name |    classification   |        score        |\n+-----------+---------------------+---------------------+\n|  nene.jpg | Branta sandvicensis |  0.7710946798324585 |\n|  nene.jpg |    Rhea americana   | 0.10109006613492966 |\n|  nene.jpg |   Branta leucopsis  |  0.0416080504655838 |\n+-----------+---------------------+---------------------+\n</code></pre> Advantages of this approach:</p> <ul> <li>Prediction accuracy: The predictions are more accurate because they are limited to species that are known to be present in the geographic region.</li> </ul> <p>Disadvantages of this approach:</p> <ul> <li>Performance: The prediction is slow because the embeddings must be calculated for all species in the list. See the following examples for approaches to speed up prediction.</li> <li>Potential misses: The predictions are limited to the species in the list, which may not include the correct species if it is an anomalous occurrence.</li> </ul> <p>Example: Predict the species of an image by combining the geographically restricted list with the precomputed embeddings of the TOL subset of taxa.</p> <p>Obtain the TOL subset. <pre><code>bioclip list-tol-taxa &gt; tol_subset.csv\n</code></pre></p> <p>Filter the TOL subset to include only the geographically restricted list in hawaii_bird_species_list.txt. For example, with Pandas in Python: <pre><code>import pandas as pd\n\n# Load the TOL subset.\ndf_tol = pd.read_csv('tol_subset.csv')\n\n# Load the list of species.\ndf_species = pd.read_csv('hawaii_bird_species_list.txt', header=None, names=['species'])\n\n# Merge the TOL subset with the list of species, inner merge\ndf_merged = pd.merge(df_tol, df_species, on='species')\n\n# Save the merged list to a file to use with the --subset option.\ndf_merged[\"species\"].to_csv(\"tol_subset_species_filtered.csv\", index=False, header=True)\n</code></pre></p> <p>Predict using the filtered TOL subset. <pre><code>bioclip predict --k 3 --format table --subset tol_subset_species_filtered.csv nene.jpg\n</code></pre> Output: <pre><code>+-----------+----------+----------+-------+--------------+----------+--------------+-----------------+---------------------+------------------+---------------------+\n| file_name | kingdom  |  phylum  | class |    order     |  family  |    genus     | species_epithet |       species       |   common_name    |       \nscore        |\n+-----------+----------+----------+-------+--------------+----------+--------------+-----------------+---------------------+------------------+---------------------+\n|  nene.jpg | Animalia | Chordata |  Aves | Anseriformes | Anatidae |    Branta    |   sandvicensis  | Branta sandvicensis |  Hawaiian Goose  |  0.7040964365005493 |\n|  nene.jpg | Animalia | Chordata |  Aves |  Rheiformes  | Rheidae  |     Rhea     |    americana    |    Rhea americana   |   Greater Rhea   | 0.18546675145626068 |\n|  nene.jpg | Animalia | Chordata |  Aves |  Gruiformes  | Gruidae  | Anthropoides |      virgo      |  Anthropoides virgo | Demoiselle Crane | 0.04178838059306145 |\n+-----------+----------+----------+-------+--------------+----------+--------------+-----------------+---------------------+------------------+---------------------+\n</code></pre></p> <p>Advantages of this approach:</p> <ul> <li>Performance: Significant speedup in prediction time because the embeddings are precomputed.</li> <li>Prediction accuracy: The predictions are more accurate than open-ended prediction alone because they are limited to species that are known to be present in the geographic region.</li> </ul> <p>Disadvantages of this approach:</p> <ul> <li>Potential misses: Only taxa in the TOL subset can be included in the geographically restricted list, which may exclude entries present in the external list.</li> </ul> <p>Example: Predict the species of an image combining open-ended classification with the geographically restricted list using custom labels.</p> <p>This approach will produce two outputs, an unconstrained open-ended prediction and a prediction constrained by the geographically restricted list for comparison.</p> <p>Predict the top 20 species with open-ended classification, saving the predictions to a file. <pre><code>bioclip predict --k 20 nene.jpg &gt; bioclip_species_predictions_oe.csv\n</code></pre> Filter the predictions using the geographically restricted list of species in hawaii_bird_species_list.txt. E.g. using Pandas in Python. <pre><code>import pandas as pd\n\n# Load the open-ended predictions.\ndf_predictions = pd.read_csv('bioclip_species_predictions_oe.csv')\n\n# Load the geographically restricted list of species.\ndf_hawaii_species = pd.read_csv('hawaii_bird_species_list.txt', header=None, names=['species'])\n\n# Filter the predictions to include only species in the geographically restricted list.\ndf_merged = pd.merge(df_predictions, df_hawaii_species, on='species', how='inner')\n\n# Save the filtered list to a file to use with the --cls option.\ndf_merged[\"species\"].to_csv(\"bioclip_species_predictions_filtered.txt\", index=False, header=False)\n</code></pre> In this example, this step removes the option to predict the crane species Grus grus and Grus communis, which are not found in Hawai'i.</p> <p>Predict the species using the filtered list. <pre><code>bioclip predict --k 3 --format table --cls bioclip_species_predictions_filtered.txt nene.jpg\n</code></pre> Output: <pre><code>+-----------+---------------------+---------------------+\n| file_name |    classification   |        score        |\n+-----------+---------------------+---------------------+\n|  nene.jpg | Branta sandvicensis |  0.8363304734230042 |\n|  nene.jpg |    Rhea americana   | 0.10964243859052658 |\n|  nene.jpg |   Anser cygnoides   | 0.02564687840640545 |\n+-----------+---------------------+---------------------+\n</code></pre> Advantages of this approach:</p> <ul> <li>Anomaly detection: In the case of an anomalous occurrence--a true sighting of an organism that has not been recorded in that region previously--we have BioCLIP's first best guess handy in <code>bioclip_species_predictions_oe.csv</code>, which is not constrained by the geographical list. This may be useful for flagging a novel invasive species or otherwise undocumented sighting.</li> <li>Performance: The long geographically restricted list is reasonably pruned, which speeds up prediction, giving <code>bioclip_species_predictions_filtered.txt</code>.</li> <li>Balanced approach: This may be a good balanced approach if there is uncertainty about the origin of the organism in the image.</li> </ul> <p>Disadvantages of this approach:</p> <ul> <li>Potential misses: The primary filter is the open-ended classification, which may not include the correct prediction if <code>-k</code> is set too low or in challenging cases.</li> </ul> <p>Example: Predict the family of an image among bird families in Hawai'i as custom labels.</p> <p>Using hawaii_bird_families_list.txt: <pre><code>bioclip predict --k 3 --format table --cls hawaii_bird_families_list.txt nene.jpg\n</code></pre> Output: <pre><code>+-----------+----------------+---------------------+\n| file_name | classification |        score        |\n+-----------+----------------+---------------------+\n|  nene.jpg |    Anatidae    | 0.34369930624961853 |\n|  nene.jpg |   Ciconiidae   | 0.28970828652381897 |\n|  nene.jpg |    Gruidae     | 0.17063161730766296 |\n+-----------+----------------+---------------------+\n</code></pre></p> <p>Example: Predict a custom binned classification of an image using the binned list of families.</p> <p>Using hawaii_bird_family_bins_list.csv: <pre><code>bioclip predict --k 3 --format table --bins hawaii_bird_family_bins_list.csv nene.jpg\n</code></pre> Output: <pre><code>+-----------+-----------------------+---------------------+\n| file_name |     classification    |        score        |\n+-----------+-----------------------+---------------------+\n|  nene.jpg |       Waterfowl       |  0.3437104821205139 |\n|  nene.jpg | Shorebirds/Waterbirds |  0.3415525555610657 |\n|  nene.jpg |         Cranes        | 0.17063717544078827 |\n+-----------+-----------------------+---------------------+\n</code></pre></p> <p>Example: Predict the order rank of an image by combining the geographically restricted list with the precomputed embeddings of the TOL subset of taxa.</p> <p>This example is similar to the earlier example using the TOL subset, but using a higher taxonomic rank with this approach has some considerations to keep in mind, as we will explore.</p> <p>This example also requires you to preprocess a list of custom labels yourself. </p> <p>To do so, download the GBIF filtered species list prepared at https://doi.org/10.15468/dl.469rtz. Extract the file <code>0001260-250123221155621.csv</code> to your working directory. Note that the delimiter in this file is the tab character (<code>\\t</code>). </p> <p>Retrieve the TOL subset. <pre><code>bioclip list-tol-taxa &gt; tol_subset.csv\n</code></pre> Filter the TOL subset to include only the geographically restricted list of bird orders in <code>0001260-250123221155621.csv</code>. For example, with Pandas in Python:  <pre><code>import pandas as pd\n\n# Load the TOL subset.\ndf_tol = pd.read_csv('tol_subset.csv')\n\n# Load the CSV from GBIF containing taxonomic data of bird species in Hawai'i.\ndf_gbif = pd.read_csv('0001260-250123221155621.csv', delimiter='\\t')\n\n# Filter the GBIF list to include only bird orders.\ndf_gbif_orders = df_gbif['order'].drop_duplicates()\n\n# Merge the orders from the TOL subset with the orders from GBIF. inner merge\ndf_merged = pd.merge(df_tol, df_gbif_orders, on='order', how='inner')\n\n# Save the merged list to a file to use with the --subset option.\ndf_merged[\"order\"].drop_duplicates().to_csv(\"tol_subset_orders_filtered.csv\", index=False, header=True)\n</code></pre></p> <p>Predict using the filtered TOL subset. <pre><code>bioclip predict --k 3 --format table --subset tol_subset_orders_filtered.csv nene.jpg\n</code></pre> Output: <pre><code>+-----------+----------+----------+-------+--------------+----------+--------+-----------------+---------------------+----------------+---------------------+\n| file_name | kingdom  |  phylum  | class |    order     |  family  | genus  | species_epithet |       species       |  common_name   |    \n    score        |\n+-----------+----------+----------+-------+--------------+----------+--------+-----------------+---------------------+----------------+---------------------+\n|  nene.jpg | Animalia | Chordata |  Aves |  Gruiformes  | Gruidae  |  Grus  |       grus      |      Grus grus      |  Common Crane  |  0.7614361047744751 |\n|  nene.jpg | Animalia | Chordata |  Aves |  Gruiformes  | Gruidae  |  Grus  |     communis    |    Grus communis    |                | 0.07306931912899017 |\n|  nene.jpg | Animalia | Chordata |  Aves | Anseriformes | Anatidae | Branta |   sandvicensis  | Branta sandvicensis | Hawaiian Goose | 0.04188006371259689 |\n+-----------+----------+----------+-------+--------------+----------+--------+-----------------+---------------------+----------------+---------------------+\n</code></pre></p> <p>Incorrect top predictions: when not to use <code>--subset</code> to speed up prediction.</p> <p>There are a few things to note about this output:</p> <ul> <li>The top predictions are incorrect as with the open-ended classification example.</li> <li>The prediction is specified to the species rank though our <code>--subset</code> file contains orders. This is expected behavior.<ul> <li>All of the species (and orders) that are not known by GBIF to be present in Hawai'i are excluded from <code>tol_subset_orders_filtered.csv</code>--however, filtering by higher-rank taxa does not eliminate non-local species. That is, even though Grus grus and Grus communis (members of order Gruiformes, i.e. \"crane-like\") are not found in Hawai'i, other members of the order Gruiformes are found in Hawai'i according to GBIF. Since the <code>--subset</code> method predicts to species level, these species are included in the prediction.</li> <li>In other words, if just one species of a higher-rank taxon is found in the geographic region, all species of that taxon will be included in the prediction when using <code>--subset</code>.</li> </ul> </li> <li>In this case, it is not advisable to use higher taxa in the geographically restricted list for use with <code>--subset</code> without additional filtering.</li> </ul> <p>If you know that the organism in your image is not from the order Gruiformes (and others), you can exclude that term (and others) from the filtered list. This will improve the accuracy of the predictions.</p> <p>In Python: <pre><code># Following the merge step between the TOL subset and the GBIF orders, remove the term Gruiformes from the list.\ndf_merged = df_merged[df_merged['order'] != 'Gruiformes']\n\n# Save the filtered list to a file to use with the --subset option.\ndf_merged[\"order\"].drop_duplicates().to_csv(\"tol_subset_orders_filtered.csv\", index=False, header=True)\n</code></pre> Predicting with the new filtered list: <pre><code>bioclip predict --k 3 --format table --subset tol_subset_orders_filtered.csv nene.jpg\n</code></pre> Output: <pre><code>+-----------+----------+----------+-------+--------------+-------------+--------------+-----------------+-----------------------+----------------+---------------------+\n| file_name | kingdom  |  phylum  | class |    order     |    family   |    genus     | species_epithet |        species        |  common_name   |        score        |\n+-----------+----------+----------+-------+--------------+-------------+--------------+-----------------+-----------------------+----------------+---------------------+\n|  nene.jpg | Animalia | Chordata |  Aves | Anseriformes |   Anatidae  |    Branta    |   sandvicensis  |  Branta sandvicensis  | Hawaiian Goose |  0.4851439893245697 |\n|  nene.jpg | Animalia | Chordata |  Aves |  Rheiformes  |   Rheidae   |     Rhea     |    americana    |     Rhea americana    |  Greater Rhea  | 0.12779226899147034 |\n|  nene.jpg | Animalia | Chordata |  Aves | Galliformes  | Phasianidae | Tetraogallus |     altaicus    | Tetraogallus altaicus | Altai Snowcock | 0.10936115682125092 |\n+-----------+----------+----------+-------+--------------+-------------+--------------+-----------------+-----------------------+----------------+---------------------+\n</code></pre></p> <p>It could be sensible to use <code>--subset</code> with higher taxa if:</p> <ul> <li>You want to speed up predictions by using precomputed embeddings.</li> <li>You are confident that the species in the higher taxa are all found in the filtered list of higher taxa labels (e.g. if you know it belongs to a certain order but some other order has species that look confusingly similar, you can exclude the other order from the list).</li> </ul>"},{"location":"python-api/","title":"Python API","text":"<ul> <li><code>KINGDOM</code></li> <li><code>PHYLUM</code></li> <li><code>CLASS</code></li> <li><code>ORDER</code></li> <li><code>FAMILY</code></li> <li><code>GENUS</code></li> <li><code>SPECIES</code></li> </ul>"},{"location":"python-api/#bioclip.TreeOfLifeClassifier","title":"<code>bioclip.TreeOfLifeClassifier(**kwargs)</code>","text":"<p>               Bases: <code>BaseClassifier</code></p> <p>A classifier for predicting taxonomic ranks for images.</p> <p>See <code>BaseClassifier</code> for details on <code>**kwargs</code>.</p> Source code in <code>src/bioclip/predict.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    See `BaseClassifier` for details on `**kwargs`.\n    \"\"\"\n    super().__init__(**kwargs)\n    self.txt_embeddings = self.get_txt_emb().to(self.device)\n    self.txt_names = self.get_txt_names()\n    self._subset_txt_embeddings = None\n    self._subset_txt_names = None\n</code></pre>"},{"location":"python-api/#bioclip.TreeOfLifeClassifier.predict","title":"<code>predict(images, rank, min_prob=1e-09, k=5, batch_size=10)</code>","text":"<p>Predicts probabilities for supplied taxa rank for given images using the Tree of Life embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>List[str] | str | List[Image]</code> <p>A list of image file paths, a single image file path, or a list of PIL Image objects.</p> required <code>rank</code> <code>Rank</code> <p>The rank at which to make predictions (e.g., species, genus).</p> required <code>min_prob</code> <code>float</code> <p>The minimum probability threshold for predictions.</p> <code>1e-09</code> <code>k</code> <code>int</code> <p>The number of top predictions to return.</p> <code>5</code> <code>batch_size</code> <code>int</code> <p>The number of images to process in a batch.</p> <code>10</code> <p>Returns:</p> Type Description <code>dict[str, dict[str, float]]</code> <p>List[dict]: A list of dicts with keys \"file_name\", taxon ranks, \"common_name\", and \"score\".</p> Source code in <code>src/bioclip/predict.py</code> <pre><code>@torch.no_grad()\ndef predict(self, images: List[str] | str | List[PIL.Image.Image], rank: Rank, \n            min_prob: float = 1e-9, k: int = 5, batch_size: int = 10) -&gt; dict[str, dict[str, float]]:\n    \"\"\"\n    Predicts probabilities for supplied taxa rank for given images using the Tree of Life embeddings.\n\n    Parameters:\n        images (List[str] | str | List[PIL.Image.Image]): A list of image file paths, a single image file path, or a list of PIL Image objects.\n        rank (Rank): The rank at which to make predictions (e.g., species, genus).\n        min_prob (float, optional): The minimum probability threshold for predictions.\n        k (int, optional): The number of top predictions to return.\n        batch_size (int, optional): The number of images to process in a batch.\n\n    Returns:\n        List[dict]: A list of dicts with keys \"file_name\", taxon ranks, \"common_name\", and \"score\".\n    \"\"\"\n\n    if isinstance(images, str):\n        images = [images]\n    probs = self.create_batched_probabilities_for_images(images, self.get_txt_embeddings(),\n                                                         batch_size=batch_size)\n    result = []\n    for i, image in enumerate(images):\n        key = self.make_key(image, i)\n        image_probs = probs[key].cpu()\n        if rank == Rank.SPECIES:\n            result.extend(self.format_species_probs(key, image_probs, k))\n        else:\n            result.extend(self.format_grouped_probs(key, image_probs, rank, min_prob, k))\n    self.record_event(images=images, rank=rank.get_label(), min_prob=min_prob, k=k, batch_size=batch_size)\n    return result\n</code></pre>"},{"location":"python-api/#bioclip.TreeOfLifeClassifier.get_label_data","title":"<code>get_label_data()</code>","text":"<p>Retrieves label data for the tree of life embeddings as a pandas DataFrame.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing label data for TOL embeddings.</p> Source code in <code>src/bioclip/predict.py</code> <pre><code>def get_label_data(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Retrieves label data for the tree of life embeddings as a pandas DataFrame.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing label data for TOL embeddings.\n    \"\"\"\n\n    data = []\n    for name_ary in self.txt_names:\n        data.append(create_classification_dict(names=name_ary, rank=Rank.SPECIES))\n    return pd.DataFrame(data, copy=True)\n</code></pre>"},{"location":"python-api/#bioclip.TreeOfLifeClassifier.create_taxa_filter","title":"<code>create_taxa_filter(rank, user_values)</code>","text":"<p>Creates a filter for taxa based on the specified rank and user-provided values.</p> <p>Parameters:</p> Name Type Description Default <code>rank</code> <code>Rank</code> <p>The taxonomic rank to filter by.</p> required <code>user_values</code> <code>List[str]</code> <p>A list of user-provided values to filter the taxa.</p> required <p>Returns:</p> Type Description <code>List[bool]</code> <p>List[bool]: A list of boolean values indicating whether each entry in the          label data matches any of the user-provided values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any of the user-provided values are not found in the label data          for the specified taxonomic rank.</p> Source code in <code>src/bioclip/predict.py</code> <pre><code>def create_taxa_filter(self, rank: Rank, user_values: List[str]) -&gt; List[bool]:\n    \"\"\"\n    Creates a filter for taxa based on the specified rank and user-provided values.\n\n    Args:\n        rank (Rank): The taxonomic rank to filter by.\n        user_values (List[str]): A list of user-provided values to filter the taxa.\n\n    Returns:\n        List[bool]: A list of boolean values indicating whether each entry in the \n                    label data matches any of the user-provided values.\n\n    Raises:\n        ValueError: If any of the user-provided values are not found in the label data \n                    for the specified taxonomic rank.\n    \"\"\"\n\n    taxa_column = rank.get_label()\n    label_data = self.get_label_data()\n\n    # Ensure all user values exist\n    pd_user_values = pd.Series(user_values, name=taxa_column)\n    unknown_values = pd_user_values[~pd_user_values.isin(label_data[taxa_column])]\n    if not unknown_values.empty:\n        bad_species = \", \".join(unknown_values.values)\n        raise ValueError(f\"Unknown {taxa_column} received: {bad_species}. Only known {taxa_column} may be used.\")\n\n    return label_data[taxa_column].isin(pd_user_values)\n</code></pre>"},{"location":"python-api/#bioclip.TreeOfLifeClassifier.apply_filter","title":"<code>apply_filter(keep_labels_ary)</code>","text":"<p>Filters the TOL embeddings based on the provided boolean array. See <code>create_taxa_filter()</code> for an easy way to create this parameter.</p> <p>Parameters:</p> Name Type Description Default <code>keep_labels_ary</code> <code>List[bool]</code> <p>A list of boolean values indicating which                            TOL embeddings to keep.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the length of keep_labels_ary does not match the expected length.</p> Source code in <code>src/bioclip/predict.py</code> <pre><code>def apply_filter(self, keep_labels_ary: List[bool]):\n    \"\"\"\n    Filters the TOL embeddings based on the provided boolean array. See `create_taxa_filter()` for an easy way to create this parameter.\n\n    Args:\n        keep_labels_ary (List[bool]): A list of boolean values indicating which \n                                      TOL embeddings to keep.\n\n    Raises:\n        ValueError: If the length of keep_labels_ary does not match the expected length.\n    \"\"\"\n\n    if len(keep_labels_ary) != len(self.txt_names):\n        expected = len(self.txt_names)\n        raise ValueError(\"Invalid keep_embeddings values. \" + \n                         f\"This parameter should be a list containing {expected} items.\")\n    embeddings = []\n    names = []\n    for idx, keep in enumerate(keep_labels_ary):\n        if keep:\n            embeddings.append(self.txt_embeddings[:,idx])\n            names.append(self.txt_names[idx])\n    self._subset_txt_embeddings = torch.stack(embeddings, dim=1)\n    self._subset_txt_names = names\n</code></pre>"},{"location":"python-api/#bioclip.Rank","title":"<code>bioclip.Rank</code>","text":"<p>Rank for the Tree of Life classification.</p>"},{"location":"python-api/#bioclip.CustomLabelsClassifier","title":"<code>bioclip.CustomLabelsClassifier(cls_ary, **kwargs)</code>","text":"<p>               Bases: <code>BaseClassifier</code></p> <p>A classifier that predicts from a list of custom labels for images.</p> <p>Initializes the classifier with the given class array and additional keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>cls_ary</code> <code>List[str]</code> <p>A list of class names as strings.</p> required Source code in <code>src/bioclip/predict.py</code> <pre><code>def __init__(self, cls_ary: List[str], **kwargs):\n    \"\"\"\n    Initializes the classifier with the given class array and additional keyword arguments.\n\n    Parameters:\n        cls_ary (List[str]): A list of class names as strings.\n    \"\"\"\n    super().__init__(**kwargs)\n    self.tokenizer = create_bioclip_tokenizer(self.model_str)\n    self.classes = [cls.strip() for cls in cls_ary]\n    self.txt_embeddings = self._get_txt_embeddings(self.classes)\n</code></pre>"},{"location":"python-api/#bioclip.CustomLabelsClassifier.predict","title":"<code>predict(images, k=None, batch_size=10)</code>","text":"<p>Predicts the probabilities for the given images.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>List[str] | str | List[Image]</code> <p>A list of image file paths, a single image file path, or a list of PIL Image objects.</p> required <code>k</code> <code>int</code> <p>The number of top probabilities to return. If not specified or if greater than the number of classes, all probabilities are returned.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>The number of images to process in a batch.</p> <code>10</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>List[dict]: A list of dicts with keys \"file_name\" and the custom class labels.</p> Source code in <code>src/bioclip/predict.py</code> <pre><code>@torch.no_grad()\ndef predict(self, images: List[str] | str | List[PIL.Image.Image], k: int = None,\n            batch_size: int = 10) -&gt; dict[str, float]:\n    \"\"\"\n    Predicts the probabilities for the given images.\n\n    Parameters:\n        images (List[str] | str | List[PIL.Image.Image]): A list of image file paths, a single image file path, or a list of PIL Image objects.\n        k (int, optional): The number of top probabilities to return. If not specified or if greater than the number of classes, all probabilities are returned.\n        batch_size (int, optional): The number of images to process in a batch.\n\n    Returns:\n        List[dict]: A list of dicts with keys \"file_name\" and the custom class labels.\n    \"\"\"\n    if isinstance(images, str):\n        images = [images]\n    probs = self.create_batched_probabilities_for_images(images, self.txt_embeddings,\n                                                         batch_size=batch_size)\n    result = []\n    for i, image in enumerate(images):\n        key = self.make_key(image, i)\n        img_probs = probs[key]\n        if not k or k &gt; len(self.classes):\n            k = len(self.classes)\n        result.extend(self.group_probs(key, img_probs, k))\n\n    self.record_event(images=images, k=k, batch_size=batch_size)\n    return result\n</code></pre>"},{"location":"python-api/#bioclip.CustomLabelsBinningClassifier","title":"<code>bioclip.CustomLabelsBinningClassifier(cls_to_bin, **kwargs)</code>","text":"<p>               Bases: <code>CustomLabelsClassifier</code></p> <p>A classifier that creates predictions for images based on custom labels, groups the labels, and calculates probabilities for each group.</p> <p>Initializes the class with a dictionary mapping class labels to binary values.</p> <p>Parameters:</p> Name Type Description Default <code>cls_to_bin</code> <code>dict</code> <p>A dictionary where keys are class labels and values are binary values.</p> required <code>**kwargs</code> <p>Additional keyword arguments passed to the superclass initializer.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any value in <code>cls_to_bin</code> is empty, null, or NaN.</p> Source code in <code>src/bioclip/predict.py</code> <pre><code>def __init__(self, cls_to_bin: dict, **kwargs):\n    \"\"\"\n    Initializes the class with a dictionary mapping class labels to binary values.\n\n    Args:\n        cls_to_bin (dict): A dictionary where keys are class labels and values are binary values.\n        **kwargs: Additional keyword arguments passed to the superclass initializer.\n\n    Raises:\n        ValueError: If any value in `cls_to_bin` is empty, null, or NaN.\n    \"\"\"\n    super().__init__(cls_ary=cls_to_bin.keys(), **kwargs)\n    self.cls_to_bin = cls_to_bin\n    if any([pd.isna(x) or not x for x in cls_to_bin.values()]):\n        raise ValueError(\"Empty, null, or nan are not allowed for bin values.\")\n</code></pre>"},{"location":"python-api/#bioclip.predict.BaseClassifier","title":"<code>bioclip.predict.BaseClassifier(model_str=BIOCLIP_MODEL_STR, pretrained_str=None, device='cpu')</code>","text":"<p>               Bases: <code>Module</code></p> <p>Initializes the prediction model.</p> <p>Parameters:</p> Name Type Description Default <code>model_str</code> <code>str</code> <p>The string identifier for the model to be used (defaults to BIOCLIP_MODEL_STR).</p> <code>BIOCLIP_MODEL_STR</code> <code>pretrained_str</code> <code>str</code> <p>The string identifier for the pretrained model to be loaded.</p> <code>None</code> <code>device</code> <code>Union[str, device]</code> <p>The device on which the model will be run.</p> <code>'cpu'</code> Source code in <code>src/bioclip/predict.py</code> <pre><code>def __init__(self, model_str: str = BIOCLIP_MODEL_STR, pretrained_str: str | None = None, device: Union[str, torch.device] = 'cpu'):\n    \"\"\"\n    Initializes the prediction model.\n\n    Parameters:\n        model_str (str): The string identifier for the model to be used (defaults to BIOCLIP_MODEL_STR).\n        pretrained_str (str, optional): The string identifier for the pretrained model to be loaded.\n        device (Union[str, torch.device]): The device on which the model will be run.\n    \"\"\"\n    super().__init__()\n    self.device = device\n    self.load_pretrained_model(model_str=model_str, pretrained_str=pretrained_str)\n    self.recorder = None\n</code></pre>"},{"location":"python-api/#bioclip.predict.BaseClassifier.forward","title":"<code>forward(x)</code>","text":"<p>Given an input tensor representing multiple images, return probabilities for each class for each image. Args:     x (torch.Tensor): Input tensor representing the multiple images. Returns:     torch.Tensor: Softmax probabilities of the logits for each class for each image.</p> Source code in <code>src/bioclip/predict.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Given an input tensor representing multiple images, return probabilities for each class for each image.\n    Args:\n        x (torch.Tensor): Input tensor representing the multiple images.\n    Returns:\n        torch.Tensor: Softmax probabilities of the logits for each class for each image.\n    \"\"\"\n    img_features = self.model.encode_image(x)\n    img_features = F.normalize(img_features, dim=-1)\n    return self.create_probabilities(img_features, self.txt_embeddings)\n</code></pre>"},{"location":"python-api/#bioclip.predict.BaseClassifier.get_cached_datafile","title":"<code>get_cached_datafile(filename)</code>","text":"<p>Downloads a datafile from the Hugging Face hub and caches it locally. Args:     filename (str): The name of the file to download from the datafile repository. Returns:     str: The local path to the downloaded file.</p> Source code in <code>src/bioclip/predict.py</code> <pre><code>def get_cached_datafile(self, filename: str) -&gt; str:\n    \"\"\"\n    Downloads a datafile from the Hugging Face hub and caches it locally.\n    Args:\n        filename (str): The name of the file to download from the datafile repository.\n    Returns:\n        str: The local path to the downloaded file.\n    \"\"\"\n    return hf_hub_download(repo_id=self.get_tol_repo_id(), filename=filename, repo_type=HF_DATAFILE_REPO_TYPE)\n</code></pre>"},{"location":"python-api/#bioclip.predict.BaseClassifier.get_tol_repo_id","title":"<code>get_tol_repo_id()</code>","text":"<p>Returns the repository ID for the TreeOfLife datafile based on the model string. Raises:     ValueError: If the model string is not supported. Returns:     str: The Hugging Face repository ID for the TreeOfLife embeddings.</p> Source code in <code>src/bioclip/predict.py</code> <pre><code>def get_tol_repo_id(self) -&gt; str:\n    \"\"\"\n    Returns the repository ID for the TreeOfLife datafile based on the model string.\n    Raises:\n        ValueError: If the model string is not supported.\n    Returns:\n        str: The Hugging Face repository ID for the TreeOfLife embeddings.\n    \"\"\"\n    return get_tol_repo_id(self.model_str)\n</code></pre>"},{"location":"python-api/#bioclip.predict.BaseClassifier.get_txt_emb","title":"<code>get_txt_emb()</code>","text":"<p>Retrieves TreeOfLife text embeddings for the current model from the associated Hugging Face dataset repo. Returns:     torch.Tensor: A tensor containing the text embeddings for the tree of life.</p> Source code in <code>src/bioclip/predict.py</code> <pre><code>def get_txt_emb(self) -&gt; torch.Tensor:\n    \"\"\"\n    Retrieves TreeOfLife text embeddings for the current model from the associated Hugging Face dataset repo.\n    Returns:\n        torch.Tensor: A tensor containing the text embeddings for the tree of life.\n    \"\"\"\n    txt_emb_npy = self.get_cached_datafile(\"embeddings/txt_emb_species.npy\")\n    return torch.from_numpy(np.load(txt_emb_npy))\n</code></pre>"},{"location":"python-api/#bioclip.predict.BaseClassifier.get_txt_names","title":"<code>get_txt_names()</code>","text":"<p>Retrieves TreeOfLife text names for the current model from the  associated Hugging Face dataset repo. Returns:     List[List[str]]: A list of lists, where each inner list contains names corresponding to the text embeddings.</p> Source code in <code>src/bioclip/predict.py</code> <pre><code>def get_txt_names(self) -&gt; List[List[str]]:\n    \"\"\"\n    Retrieves TreeOfLife text names for the current model from the  associated Hugging Face dataset repo.\n    Returns:\n        List[List[str]]: A list of lists, where each inner list contains names corresponding to the text embeddings.\n    \"\"\"\n    txt_names_json = self.get_cached_datafile(\"embeddings/txt_emb_species.json\")\n    with open(txt_names_json) as fd:\n        txt_names = json.load(fd)\n    return txt_names\n</code></pre>"},{"location":"python-api/#bioclip.recorder","title":"<code>bioclip.recorder</code>","text":"<p>Records predictions made by a classifier and saves the output to a file.</p>"},{"location":"python-api/#bioclip.recorder.attach_prediction_recorder","title":"<code>attach_prediction_recorder(classifier, **top_level_settings)</code>","text":"<p>Attach a PredictionRecorder to the classifier instance that will record metadata and subsequent predictions. Call save_recorded_predictions to save the recorded predictions to a file.</p> <p>Parameters:</p> Name Type Description Default <code>classifier</code> <code>object</code> <p>The classifier (such as TreeOfLifeClassifier) instance to attach the recorder to.</p> required <code>**top_level_settings</code> <p>Additional settings to be recorded.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>PredictionRecorder</code> <p>An instance of PredictionRecorder attached to the classifier.</p> Source code in <code>src/bioclip/recorder.py</code> <pre><code>def attach_prediction_recorder(classifier: object, **top_level_settings):\n    \"\"\"\n    Attach a PredictionRecorder to the classifier instance that will record metadata and subsequent predictions.\n    Call save_recorded_predictions to save the recorded predictions to a file.\n\n    Args:\n        classifier (object): The classifier (such as TreeOfLifeClassifier) instance to attach the recorder to.\n        **top_level_settings: Additional settings to be recorded.\n\n    Returns:\n        PredictionRecorder: An instance of PredictionRecorder attached to the classifier.\n    \"\"\"\n    recorder = PredictionRecorder(classifier, **top_level_settings)\n    classifier.set_recorder(recorder)\n    return recorder\n</code></pre>"},{"location":"python-api/#bioclip.recorder.save_recorded_predictions","title":"<code>save_recorded_predictions(classifier, path, include_command_line=True)</code>","text":"<p>Saves recorded predictions from the classifier to a file. Before calling this function, ensure that the classifier has a recorder attached using attach_prediction_recorder. Saves the recorder's data to the specified file path in  either JSON or plain text format. If the file extension is '.json', the data is serialized as JSON. Otherwise, the data is appended in a human-readable text format.</p> <p>Parameters:</p> Name Type Description Default <code>classifier</code> <code>object</code> <p>The classifier instance (such as TreeOfLifeClassifier) with recorded predictions.</p> required <code>path</code> <code>str</code> <p>The file path where the report will be saved.</p> required <code>include_command_line</code> <code>bool</code> <p>When True includes the python command line in the log file.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the output path extension is .json and the file already exists.</p> Source code in <code>src/bioclip/recorder.py</code> <pre><code>def save_recorded_predictions(classifier: object, path: str, include_command_line: bool = True):\n    \"\"\"\n    Saves recorded predictions from the classifier to a file.\n    Before calling this function, ensure that the classifier has a recorder attached\n    using attach_prediction_recorder. Saves the recorder's data to the specified file path in \n    either JSON or plain text format. If the file extension is '.json', the data is serialized\n    as JSON. Otherwise, the data is appended in a human-readable text format.\n\n    Args:\n        classifier (object): The classifier instance (such as TreeOfLifeClassifier) with recorded predictions.\n        path (str): The file path where the report will be saved.\n        include_command_line (bool): When True includes the python command line in the log file.\n\n    Raises:\n        ValueError: If the output path extension is .json and the file already exists.\n    \"\"\"\n    if classifier.recorder:\n        command_line = \" \".join(sys.argv) if include_command_line else None\n        classifier.recorder.create_report(path, command_line=command_line)\n    else:\n        raise ValueError(\"The classifier does not have a recorder attached.\")\n</code></pre>"},{"location":"python-tutorial/","title":"Python Tutorial","text":"<p>Before beginning this tutorial you need to install pybioclip and download two example images: <code>Ursus-arctos.jpeg</code>  and <code>Felis-catus.jpeg</code>.</p>"},{"location":"python-tutorial/#predict-species-classification","title":"Predict species classification","text":"<pre><code>from bioclip import TreeOfLifeClassifier, Rank\n\nclassifier = TreeOfLifeClassifier()\npredictions = classifier.predict(\"Ursus-arctos.jpeg\", Rank.SPECIES)\n\nfor prediction in predictions:\n    print(prediction[\"species\"], \"-\", prediction[\"score\"])\n</code></pre> <p>Output: <pre><code>Ursus arctos - 0.9356034994125366\nUrsus arctos syriacus - 0.05616999790072441\nUrsus arctos bruinosus - 0.004126196261495352\nUrsus arctus - 0.0024959812872111797\nUrsus americanus - 0.0005009894957765937\n</code></pre></p> <p>Output from the <code>predict()</code> method showing the dictionary structure: <pre><code>[{\n    'kingdom': 'Animalia',\n    'phylum': 'Chordata',\n    'class': 'Mammalia',\n    'order': 'Carnivora',\n    'family': 'Ursidae',\n    'genus': 'Ursus',\n    'species_epithet': 'arctos',\n    'species': 'Ursus arctos',\n    'common_name': 'Kodiak bear'\n    'score': 0.9356034994125366\n}]\n</code></pre></p> <p>Model and Embedding config</p> <p>By default the predictions use the BioCLIP 2 model in combination with its TreeOfLife-200M embeddings. By setting the <code>model_str</code> parameter to <code>bioclip.BIOCLIP_V1_MODEL_STR</code> the original BioCLIP model will be used in combination with its TreeOfLife-10M embeddings.</p> <p>The output from the predict function can be converted into a pandas DataFrame like so: <pre><code>import pandas as pd\nfrom bioclip import TreeOfLifeClassifier, Rank\n\nclassifier = TreeOfLifeClassifier()\npredictions = classifier.predict(\"Ursus-arctos.jpeg\", Rank.SPECIES)\ndf = pd.DataFrame(predictions)\n</code></pre></p> <p>The first argument of the <code>predict()</code> method supports both a single path or a list of paths.</p> <p>Documentation</p> <p>The TreeOfLifeClassifier docs contains details about the arguments supported by the constructor and the <code>predict()</code> method.</p>"},{"location":"python-tutorial/#predict-from-a-list-of-classes","title":"Predict from a list of classes","text":"<p><pre><code>from bioclip import CustomLabelsClassifier\n\nclassifier = CustomLabelsClassifier([\"duck\",\"fish\",\"bear\"])\npredictions = classifier.predict(\"Ursus-arctos.jpeg\")\nfor prediction in predictions:\n   print(prediction[\"classification\"], prediction[\"score\"])\n</code></pre> Output: <pre><code>duck 1.0306726583309e-09\nfish 2.932403668845507e-12\nbear 1.0\n</code></pre></p> <p>Documentation</p> <p>The CustomLabelsClassifier docs contains details about the arguments supported by the constructor and the <code>predict()</code> method.</p>"},{"location":"python-tutorial/#predict-using-a-custom-model","title":"Predict using a Custom Model","text":"<p>To predict with a custom model the <code>model_str</code> and <code>pretrained_str</code> arguments must be specified. In this example the CLIP-ViT-B-16-laion2B-s34B-b88K model is used. <pre><code>from bioclip import CustomLabelsClassifier\n\nclassifier = CustomLabelsClassifier(\n    cls_ary = [\"duck\",\"fish\",\"bear\"],\n    model_str='ViT-B-16',\n    pretrained_str='laion2b_s34b_b88k')\n\nprint(classifier.predict(\"Ursus-arctos.jpeg\"))\n</code></pre></p>"},{"location":"python-tutorial/#predict-with-the-original-bioclip-model","title":"Predict with the original BioCLIP model","text":"<pre><code>from bioclip import CustomLabelsClassifier, BIOCLIP_V1_MODEL_STR\n\nclassifier = CustomLabelsClassifier(\n    cls_ary = [\"duck\",\"fish\",\"bear\"],\n    model_str=BIOCLIP_V1_MODEL_STR)\n\nprint(classifier.predict(\"Ursus-arctos.jpeg\"))\n</code></pre> <p>See this tutorial for instructions for listing available pretrained models.</p>"},{"location":"python-tutorial/#predict-from-a-list-of-classes-with-binning","title":"Predict from a list of classes with binning","text":"<p><pre><code>from bioclip import CustomLabelsBinningClassifier\n\nclassifier = CustomLabelsBinningClassifier(cls_to_bin={\n  'dog': 'small',\n  'fish': 'small',\n  'bear': 'big',\n})\npredictions = classifier.predict(\"Ursus-arctos.jpeg\")\n\nfor prediction in predictions:\n   print(prediction[\"classification\"], prediction[\"score\"])\n</code></pre> Output: <pre><code>big 0.99992835521698\nsmall 7.165559509303421e-05\n</code></pre></p> <p>Documentation</p> <p>The CustomLabelsBinningClassifier documentation describes all arguments supported by the constructor. The base class CustomLabelsClassifier docs describes arguments for the predict method.</p>"},{"location":"python-tutorial/#example-notebooks","title":"Example Notebooks","text":""},{"location":"python-tutorial/#predict-species-for-images","title":"Predict species for images","text":"<p>PredictImages.ipynb  downloads some images and predicts species. </p>"},{"location":"python-tutorial/#predict-species-for-inaturalist-images","title":"Predict species for iNaturalist images","text":"<p>iNaturalistPredict.ipynb downloads images from inaturalist.org and predicts species.  </p>"},{"location":"python-tutorial/#predict-using-a-subset-of-the-treeoflife","title":"Predict using a subset of the TreeOfLife","text":"<p>TOL-Subsetting.ipynb filters the TreeOfLife embeddings.  </p> <p>Documentation</p> <p>For subsetting the TreeOfLifeClassifier see get_label_data(), create_taxa_filter() and apply_filter() .</p>"},{"location":"python-tutorial/#experiment-with-grad-cam","title":"Experiment with grad-cam","text":"<p>GradCamExperiment.ipynb  applies GradCAM AI explainability to BioCLIP.  </p>"},{"location":"python-tutorial/#fine-tune","title":"Fine-tune","text":""},{"location":"python-tutorial/#notebooks","title":"Notebooks","text":"<p>The following notebooks show methods to fine-tune BioCLIP for classification.</p> <ul> <li> <p>FineTuneSVM.ipynb fine-tunes  BioCLIP by combining an SVM with BioCLIP image embeddings.  </p> </li> <li> <p>FineTuneRidgeClassifier.ipynb fine-tunes BioCLIP by combining a RidgeClassifier with BioCLIP image embeddings.  </p> </li> <li> <p>FineTuneSimpleShot.ipynb fine-tunes BioCLIP by combining a SimpleShot classifier with BioCLIP image embeddings.  </p> </li> </ul> <p>As can be seen from comparing the confusion matrices in the notebooks, fine-tuning may yield better results than using BioCLIP in \"zero-shot mode\", i.e., predicting on a list of custom labels.</p> <p>This work is based on code from biobench.</p>"},{"location":"python-tutorial/#comparison-of-methods","title":"Comparison of methods","text":"Method Maximum Classes Minimum Training Data SVM ~20 5+ examples Ridge Classifier No maximum 10+ examples per class SimpleShot No maximum 1+ example per class <ul> <li>SVMs can support linear and non-linear boundaries and are suitable for binary classification or fewer than ~20 classes (because you train a one-vs-rest for each class).</li> <li>Ridge classifiers are best for linear classification tasks. They require training but are powerful classifiers for many, many tasks, especially with sufficient data.</li> <li>SimpleShot is extremely data-efficient and works well for multiple (20+) classes.</li> </ul>"},{"location":"python-tutorial/#pil-images","title":"PIL Images","text":"<p>The predict() functions used in all the examples above allow passing a list of paths or a list of PIL Images. When a list of PIL images is passed the index of the image will be filled in for <code>file_name</code>. This is because PIL images may not have an associated file name.</p>"}]}